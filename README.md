# RunCodeEval: A framework to evaluate LLMs trained on code using CodeEval benchmark dataset

CodeEval is an innovative, pedagogy-inspired benchmarking dataset that mirrors the evaluation processes encountered in academic programming courses. It assesses LLMs across 27 distinct aspects of Python programming at three proficiency levels: beginner, intermediate, and advanced. The RunCodeEval software framework facilitates model evaluation using the CodeEval dataset. The framework yields detailed insights into the strengths and weaknesses of code-trained models

# How to use RunCodeEval Framework
RunCodeEval is self-contained and doesn't need any external dependencies to run evaluation. In order to run evaluation, develop an input file with LLM generated solutions for to each problem in the CodeEval benchmarking. An example input file is present in [phi2_candidates.jsonl](completions/phi2_candidates.jsonl). 

To run the evaluation, run the following command - 
```
python driver.py -ft completions/phi2_candidates.jsonl runcodeeval/benchmark/codeeval_v1.jsonl phi-2
```
The first argument is the path to the input file containing the model generated solutions for each problem in the CodeEval benchmark dataset whose path is provided as second argument. The third argument is the `model name` which is used to name the directory to contain all results generated by the above command. For example, the above command generates the result files - `[test_results_score.json](results/phi-2/test_results_score.json), [test_results_score.json](results/phi-2/test_results_score.json) and [test_results.jsonl](results/phi-2/test_results.jsonl) in the folder [results/phi-2](results/phi-2)

The framework has a convenience feature to run evaluation for only one model generated solution. This can be used for debugging and random testing of model generated code. An example command that runs evaluation for an instance of CodeEval benchmark dataset identifier by `codeeval-20` is shown below - 

```
python driver.py -fto codeeval-20 completions/phi2_candidates.jsonl runcodeeval/benchmark/codeeval_v1.jsonl phi-2
```
The RunCodeEval framework has built-in log which is generated as `app.log` in the project root directory.

# CodeEval Benchmark dataset
While RunCodeFramework constitutes the [CodeEval benchmark dataset](runcodeeval/benchmark/codeeval_v1.jsonl), it can also be freely and independently downloaded from its permanent doi [link](https://doi.org/10.5281/zenodo.11100073) 
